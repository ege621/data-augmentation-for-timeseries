import numpy as np

#This step runs purely on CPU and RAM. You might get an out of memory error if you choose a high value for iterations
#Be sure to observe your ram on task manager. If it explodes, reduce the number of iterations.
#The resulting dimension of the dataset will be (iterations* original batch size , sequence length, number of features)
X = np.load('dataset/dataset_X.npy')
y = np.load('dataset/dataset_y.npy')

iterations = 5 #do the below loop this many times
mu, sigma = 0, 0.03 #the mean and the standart deviation of the noise
X_new = np.zeros((X.shape[0],X.shape[1],X.shape[2])) #create sparse arrays
y_new = np.zeros((y.shape[0],y.shape[1]))
#Maybe there is a better way to do this without for loops
for i in range(iterations):
    noise = np.random.normal(mu, sigma, [X.shape[0],X.shape[1],X.shape[2]]) #create the noise matrix for each iteration
    #the noises will have both positive and negative values, which is good to randomly add or subtract values to out original dataset
    Xt = np.multiply(noise,X) + X #multiply the noise dataset with our dataset and add it to our dataset
    X_new = np.concatenate([X_new, Xt], axis = 0) #concatenate the new matrix to itself
    print("Done with the {}th iteration".format(i)) #print which step are we in 
X_new = X_new[X.shape[0]:]    #remove the first set of data since it is all zeros

for i in range(iterations): #since the concat order is the same. the one hot vectors can be repeated in the same pattern.
    y_new = np.concatenate([y, y_new], 0)

y_new = y_new[~np.all(y_new == 0, axis=1)] #remove rows that only contain 0's

#save our augmented dataset with the values we defined above. This comes in handy if you want to do multiple
#augmentation trials and keep track of the parameters you used
#be sure to edit your dataset path accordingly
save_dirX = "dataset/" + "X_Augmented_Sigma" + str(sigma) + "_" + str(iterations) + ".npy"
save_diry = "dataset/" + "y_Augmented_Sigma" + str(sigma) + "_" + str(iterations) + ".npy"

print(X_new.shape)
print(y_new.shape)
